{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4983d1-4cf8-4c57-8e4e-c99849aa2fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from osgeo import gdal\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime  \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "48576f18-a212-43f5-a65f-1018501e33ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/SUST/Desktop/dengue4/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3593336-474f-47bb-becb-9c1240063902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## 数据预处理\n",
    "def preprocess(data_list, prepro):  \n",
    "    if prepro == 0:\n",
    "        data_list_transformed = data_list \n",
    "    elif prepro == 1: \n",
    "        data_list_transformed = preprocessing.RobustScaler().fit_transform(data_list)\n",
    "    elif prepro == 2: \n",
    "        data_list_transformed = preprocessing.MinMaxScaler().fit_transform(data_list)\n",
    "    elif prepro == 3: \n",
    "        data_list_transformed = preprocessing.MaxAbsScaler().fit_transform(data_list)\n",
    "    elif prepro == 4: \n",
    "        data_list_transformed = preprocessing.StandardScaler().fit_transform(data_list)\n",
    "    elif prepro == 5:\n",
    "        data_list_transformed = preprocessing.Normalizer().fit_transform(data_list)\n",
    "    elif prepro == 6: \n",
    "        data_list_transformed = preprocessing.FunctionTransformer(np.log1p).transform(data_list)\n",
    "    elif prepro == 7:\n",
    "        temp = preprocessing.FunctionTransformer(np.log1p).transform(data_list)\n",
    "        data_list_transformed = preprocessing.MinMaxScaler().fit_transform(temp)\n",
    "    return data_list_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369bcf60-b992-46f2-a22b-15c356915cae",
   "metadata": {},
   "source": [
    "# Five-fold cross validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a294d87c-5361-43bd-87dd-2f2f5a1e9fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"data/all/dengue.csv\"\n",
    "PRECESS_MODE = 0 \n",
    "RANDOM_STATE = 0 \n",
    "\n",
    "\n",
    "data = pd.read_csv(FILE_PATH) \n",
    "data_coords_x = np.array(data['Longitude'], dtype=np.float64)  \n",
    "data_coords_y = np.array(data['Latitude'], dtype=np.float64)  \n",
    "data_coords = list(zip(data['Longitude'], data['Latitude']))       \n",
    "data_coords = np.array(data_coords, dtype=np.float64)    \n",
    "\n",
    "name_x = ['bio1','bio5','bio6','bio12','bio13','bio14','ISA','DEM','POP','albo2020','egyp2020']  \n",
    "name_y = 'label'  \n",
    "name_all = name_x.copy()  \n",
    "name_all.append(name_y)   \n",
    "\n",
    "\n",
    "train_data = pd.read_csv(f'data/all/train.csv')\n",
    "data_xcoords_train = np.array(train_data['Longitude'], dtype=np.float64)  \n",
    "data_ycoords_train = np.array(train_data['Latitude'], dtype=np.float64)  \n",
    "data_coords_train = list(zip(train_data['Longitude'], train_data['Latitude']))     \n",
    "data_coords_train = np.array(data_coords_train, dtype=np.float64) # (5396,2)\n",
    "\n",
    "valid_data = pd.read_csv(f'data/all/valid.csv')\n",
    "data_xcoords_valid = np.array(valid_data['Longitude'], dtype=np.float64)  \n",
    "data_ycoords_valid = np.array(valid_data['Latitude'], dtype=np.float64)  \n",
    "data_coords_valid = list(zip(valid_data['Longitude'], valid_data['Latitude']))     \n",
    "data_coords_valid = np.array(data_coords_valid, dtype=np.float64) # (1350,2)\n",
    "\n",
    "x_temp = []\n",
    "for name in name_x:\n",
    "    x_temp.append(np.array(train_data[name], dtype=np.float64).reshape(-1, 1))\n",
    "x_train = np.hstack(x_temp)\n",
    "y_train = np.hstack(train_data[name_y], dtype=np.float64).reshape(-1, 1)\n",
    "\n",
    "\n",
    "x_temp = []\n",
    "for name in name_x:\n",
    "    x_temp.append(np.array(valid_data[name], dtype=np.float64).reshape(-1, 1))\n",
    "x_valid = np.hstack(x_temp) \n",
    "y_valid = np.hstack(valid_data[name_y], dtype=np.float64).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1301a6fc-5aa1-4881-a97b-d61a04d63b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FID', 'pointid', 'label', 'Longitude', 'Latitude', 'bio1', 'bio2',\n",
       "       'bio3', 'bio4', 'bio5', 'bio6', 'bio7', 'bio8', 'bio9', 'bio10',\n",
       "       'bio11', 'bio12', 'bio13', 'bio14', 'bio15', 'bio16', 'bio17', 'bio18',\n",
       "       'bio19', 'DEM', 'ISA', 'POP', 'albo2020', 'egyp2020'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2140a7e0-36fb-4e03-862e-ebf6ab4ca1d2",
   "metadata": {},
   "source": [
    "# GNNWLR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8f3ba67e-ff28-4214-882c-906c9cde85e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "feature_num = len(name_x)  \n",
    "weight_num = feature_num + 1  \n",
    "\n",
    "def get_nn_dis(sample_coords, ref_coords_x, ref_coords_y):\n",
    "    dis_list = [] \n",
    "    for i in range(sample_coords.shape[0]): \n",
    "        sample_coords_x = sample_coords[i][0] \n",
    "        sample_coords_y = sample_coords[i][1] \n",
    "        dis = np.sqrt(np.square(ref_coords_x - sample_coords_x) + np.square(ref_coords_y - sample_coords_y))\n",
    "        dis_list.append(dis)  \n",
    "    return np.array(dis_list) \n",
    "\n",
    "def my_round(x):\n",
    "    condition = tf.greater(x, 0.5) \n",
    "    return tf.where(condition, 1.0, 0.0)  \n",
    "\n",
    "\n",
    "def dis_in_gnnwlr(input_dis):\n",
    "    dis = input_dis \n",
    "\n",
    "    dis = tf.keras.layers.Dense(1000, activation='relu', kernel_initializer=tf.initializers.HeNormal(),\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(0.01))(dis)\n",
    "    dis = tf.keras.layers.Dense(512, activation='relu', kernel_initializer=tf.initializers.HeNormal(),\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(0.01))(dis)\n",
    "    dis = tf.keras.layers.Dense(256, activation='relu', kernel_initializer=tf.initializers.HeNormal(),\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(0.01))(dis)\n",
    "    dis = tf.keras.layers.Dense(64, activation='relu', kernel_initializer=tf.initializers.HeNormal(),\n",
    "                                kernel_regularizer=tf.keras.regularizers.l2(0.01))(dis)\n",
    "\n",
    "    \n",
    "    dis = tf.keras.layers.Dense(weight_num)(dis)\n",
    "    return dis\n",
    "\n",
    "x_temp = []\n",
    "for name in name_x:\n",
    "    x_temp.append(np.array(train_data[name], dtype=np.float64).reshape(-1, 1))\n",
    "x_train_demo = np.hstack(x_temp) \n",
    "dis_train_demo = get_nn_dis(data_coords_train, data_coords_x, data_coords_y) \n",
    "\n",
    "def gnnwlr(beta_ols):\n",
    "    input_dis = tf.keras.layers.Input(shape=dis_train_demo.shape[1:]) \n",
    "    inputs_x = tf.keras.layers.Input(shape=x_train_demo.shape[1:]) \n",
    "\n",
    "    dis = dis_in_gnnwlr(input_dis)\n",
    "\n",
    "    weights = [] \n",
    "    for i in range(weight_num): \n",
    "        weights.append(dis[:, i]) \n",
    "\n",
    "    y_pred = weights[0] * beta_ols[0][0] \n",
    "    for i in range(1, weight_num): \n",
    "        y_pred += weights[i] * beta_ols[i][0] * inputs_x[:, i-1] \n",
    "\n",
    "    y_bin = tf.keras.layers.Lambda(lambda z: tf.nn.sigmoid(z))(y_pred) \n",
    "    return tf.keras.Model(inputs=(input_dis, inputs_x), outputs=y_bin) \n",
    "\n",
    "# this is SWNN. Calculate the weights of input locations.\n",
    "def swnn():\n",
    "    input_dis = tf.keras.layers.Input(shape=dis_train_demo.shape[1:]) \n",
    "    inputs_x = tf.keras.layers.Input(shape=x_train_demo.shape[1:])  \n",
    "\n",
    "    dis = dis_in_gnnwlr(input_dis) \n",
    "\n",
    "    return tf.keras.Model(inputs=(input_dis, inputs_x), outputs=dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "84730703-83fb-4a4b-9453-ab04c950b313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\").replace(\"'\", \"\")\n",
    "\n",
    "nn_logdir = './mpm_nn_callbacks' \n",
    "if not os.path.exists(nn_logdir):  \n",
    "    os.mkdir(nn_logdir)\n",
    "nn_output_model_path = os.path.join(nn_logdir, \"gnnwlr_model_{}.keras\".format(TIMESTAMP)) \n",
    "\n",
    "# nn_optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001,initial_accumulator_value=0.1,epsilon=1e-07,name='Adagrad', )\n",
    "# nn_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) # 0.96 0.71 0.86 0.66\n",
    "# nn_optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.708, nesterov=True)\n",
    "nn_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001) # 0.95 0.87 0.85 0.78\n",
    "\n",
    "def nn_scheduler(epoch, lr): \n",
    "    if epoch < 80: \n",
    "        return 0.001\n",
    "    else: \n",
    "        return lr * tf.math.exp(-0.005)\n",
    "\n",
    "\n",
    "nn_callbacks = [\n",
    "    tf.keras.callbacks.TensorBoard(nn_logdir), \n",
    "    tf.keras.callbacks.ModelCheckpoint(nn_output_model_path, monitor='val_loss', save_best_only=True, mode='auto'),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.010, \n",
    "                                     patience=15, restore_best_weights=True, mode='auto') \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b7efcc-d0d2-4e17-8534-89336ff5090c",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6d1f7cb3-5b4d-4974-8bb7-770c5400c2a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Cross-Validation =====\n",
      "beta_0 in ols is 1.8374296353341393\n",
      "Epoch 1/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - auc: 0.4979 - loss: 23.8747 - val_auc: 0.8371 - val_loss: 0.8979\n",
      "Epoch 2/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - auc: 0.8604 - loss: 0.6244 - val_auc: 0.8557 - val_loss: 0.4703\n",
      "Epoch 3/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 29ms/step - auc: 0.8848 - loss: 0.4352 - val_auc: 0.8673 - val_loss: 0.4479\n",
      "Epoch 4/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - auc: 0.8715 - loss: 0.4600 - val_auc: 0.8621 - val_loss: 0.6894\n",
      "Epoch 5/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - auc: 0.8872 - loss: 0.4284 - val_auc: 0.8767 - val_loss: 0.4990\n",
      "Epoch 6/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - auc: 0.8920 - loss: 0.4185 - val_auc: 0.8299 - val_loss: 0.7811\n",
      "Epoch 7/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - auc: 0.8957 - loss: 0.4139 - val_auc: 0.8747 - val_loss: 0.5998\n",
      "Epoch 8/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - auc: 0.8918 - loss: 0.4191 - val_auc: 0.8757 - val_loss: 0.5807\n",
      "Epoch 9/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - auc: 0.8985 - loss: 0.4088 - val_auc: 0.8595 - val_loss: 0.6521\n",
      "Epoch 10/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - auc: 0.9046 - loss: 0.3940 - val_auc: 0.8720 - val_loss: 0.7263\n",
      "Epoch 11/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - auc: 0.9008 - loss: 0.4040 - val_auc: 0.8647 - val_loss: 0.7567\n",
      "Epoch 12/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - auc: 0.9008 - loss: 0.4075 - val_auc: 0.8601 - val_loss: 0.6054\n",
      "Epoch 13/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step - auc: 0.9035 - loss: 0.4017 - val_auc: 0.8644 - val_loss: 0.6306\n",
      "Epoch 14/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - auc: 0.9067 - loss: 0.3935 - val_auc: 0.8543 - val_loss: 0.6965\n",
      "Epoch 15/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - auc: 0.9039 - loss: 0.3970 - val_auc: 0.8538 - val_loss: 0.7704\n",
      "Epoch 16/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 28ms/step - auc: 0.9108 - loss: 0.3819 - val_auc: 0.8784 - val_loss: 0.5504\n",
      "Epoch 17/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - auc: 0.9123 - loss: 0.3866 - val_auc: 0.8626 - val_loss: 0.9130\n",
      "Epoch 18/500\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - auc: 0.9052 - loss: 0.4029 - val_auc: 0.8586 - val_loss: 0.6950\n",
      "\u001b[1m149/149\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m37/37\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "y train AUC: 0.8960306432603437\n",
      "y valid AUC: 0.8673166826311349\n",
      "y train ACC: 0.8041237113402062\n",
      "y valid ACC: 0.8673986486486487\n",
      "Cross Validation finished.\n"
     ]
    }
   ],
   "source": [
    "from pysal.model.spreg import OLS\n",
    "import datetime\n",
    "from sklearn import metrics\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "EPOCHS = 500 \n",
    "NN_BATCH_SIZE = 32 \n",
    "NN_BUFFER_SIZE = 4096 \n",
    "\n",
    "AUC_train_result = []\n",
    "AUC_valid_result = []\n",
    "ACC_train_result = []\n",
    "ACC_valid_result = []\n",
    "\n",
    "print(f\"====Cross-Validation =====\") # \n",
    "# clear the session\n",
    "tf.keras.backend.clear_session() \n",
    "dis_train = get_nn_dis(data_coords_train, data_coords_x, data_coords_y)\n",
    "dis_valid = get_nn_dis(data_coords_valid, data_coords_x, data_coords_y)\n",
    "\n",
    "nn_train_input = tf.data.Dataset.from_tensor_slices((dis_train, x_train))\n",
    "nn_train_output = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "nn_train = tf.data.Dataset.zip((nn_train_input, nn_train_output))\n",
    "nn_train = nn_train.shuffle(NN_BUFFER_SIZE).batch(NN_BATCH_SIZE)\n",
    "\n",
    "nn_valid_input = tf.data.Dataset.from_tensor_slices((dis_valid, x_valid))\n",
    "nn_valid_output = tf.data.Dataset.from_tensor_slices(y_valid)\n",
    "nn_valid = tf.data.Dataset.zip((nn_valid_input, nn_valid_output))\n",
    "nn_valid = nn_valid.batch(NN_BATCH_SIZE)\n",
    "\n",
    "ols = OLS(y_train, x_train, name_y=name_y, name_x=name_x)\n",
    "beta_ols = ols.betas\n",
    "print(f'beta_0 in ols is {beta_ols[0][0]}')\n",
    "\n",
    "# calling the model and compile it \n",
    "gnnwlr_model = gnnwlr(beta_ols)\n",
    "gnnwlr_model.compile(optimizer=nn_optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n",
    "\n",
    "# run the model \n",
    "nn_history = gnnwlr_model.fit(nn_train, validation_data=nn_valid, epochs=EPOCHS, callbacks=nn_callbacks)\n",
    "\n",
    "nn_y_train = gnnwlr_model.predict((dis_train, x_train)).reshape(-1, 1)\n",
    "nn_y_train_0_1 = np.where(nn_y_train > 0.5, 1, 0)\n",
    "nn_y_valid = gnnwlr_model.predict((dis_valid, x_valid)).reshape(-1, 1)\n",
    "nn_y_valid_0_1 = np.where(nn_y_valid > 0.5, 1, 0)\n",
    "    \n",
    "train_auc = metrics.roc_auc_score(y_train, nn_y_train)\n",
    "valid_auc = metrics.roc_auc_score(y_valid, nn_y_valid)\n",
    "train_acc = metrics.accuracy_score(y_train, nn_y_train_0_1)\n",
    "valid_acc = metrics.accuracy_score(y_valid, nn_y_valid_0_1)\n",
    "print('y train AUC:', train_auc)\n",
    "AUC_train_result.append(train_auc)\n",
    "print('y valid AUC:', valid_auc)\n",
    "AUC_valid_result.append(valid_auc)\n",
    "print('y train ACC:', train_acc)\n",
    "ACC_train_result.append(train_acc)\n",
    "print('y valid ACC:', valid_acc)\n",
    "ACC_valid_result.append(valid_acc)\n",
    "\n",
    "TIMESTAMP = datetime.datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\").replace(\"'\", \"\")\n",
    "pd.DataFrame(beta_ols).to_csv(f'./CV/weight/beta_ols_cv.csv', index=False) # 需要替换 i\n",
    "\n",
    "gnnwlr_model.save_weights(f'./CV/weight/gnnwlr_wg_cv.weights.h5') # 需要替换 i\n",
    "print(f'Cross Validation finished.') # 需要替换 i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7366260f-3bd3-44da-91a0-376b961dd82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1184, 1)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3e3c66-7ca9-4b2a-ab14-34a08ab56371",
   "metadata": {},
   "source": [
    "# GNNWLR prediction，ROC、AUC、ACC、Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a86460eb-4050-4a94-bc7d-282673779a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "beta_ols = np.array(pd.read_csv(f'./CV/weight/beta_ols_cv.csv'))\n",
    "gnnwlr_model = gnnwlr(beta_ols)\n",
    "gnnwlr_model.load_weights(f'./CV/weight/gnnwlr_wg_cv.weights.h5')\n",
    "\n",
    "dis_data = get_nn_dis(data_coords, data_coords_x, data_coords_y)\n",
    "\n",
    "x_temp = []\n",
    "for name in name_x:\n",
    "    x_temp.append(np.array(data[name], dtype=np.float64).reshape(-1, 1))\n",
    "x_all = np.hstack(x_temp) \n",
    "y_all = np.hstack(data[name_y], dtype=np.float64).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9ed451be-a3f6-4ed7-aeb7-502a99fbd246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m186/186\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "y all AUC: 0.8966273955644992\n",
      "all region acc: 0.8167424625231599\n",
      "all region recall: 0.837406321277289\n"
     ]
    }
   ],
   "source": [
    "nn_pred = gnnwlr_model.predict((dis_data, x_all)).reshape(-1, 1)\n",
    "\n",
    "all_auc = roc_auc_score(y_all, nn_pred)\n",
    "print('y all AUC:', all_auc)\n",
    "\n",
    "acc_all = accuracy_score(y_all, np.where(nn_pred > 0.5, 1, 0))\n",
    "recall_all = recall_score(y_all, np.where(nn_pred > 0.5, 1, 0))\n",
    "print(f'all region acc: {acc_all}')\n",
    "print(f'all region recall: {recall_all}')\n",
    "\n",
    "data['GNNWLR_pred'] = nn_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d08f1a0-fb7d-479d-bee0-c58affa34f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "beta_ols = np.array(pd.read_csv(f'./CV/weight/beta_ols_cv.csv'))\n",
    "gnnwlr_model = gnnwlr(beta_ols)\n",
    "gnnwlr_model.load_weights(f'./CV/weight/gnnwlr_wg_cv.weights.h5')\n",
    "\n",
    "data = pd.read_csv(FILE_PATH)\n",
    "x_temp = []\n",
    "for name in name_x:\n",
    "    x_temp.append(np.array(data[name], dtype=np.float64).reshape(-1, 1))\n",
    "x_data = np.hstack(x_temp) \n",
    "\n",
    "data_coords = list(zip(data['Longitude'], data['Latitude']))\n",
    "data_coords = np.array(data_coords, dtype=np.float64)\n",
    "dis_data = get_nn_dis(data_coords, data_coords_x, data_coords_y)\n",
    "\n",
    "nn_pred = gnnwlr_model.predict((dis_data, x_data)).reshape(-1, 1)\n",
    "labels = np.where(nn_pred > 0.5, 1, 0)\n",
    "\n",
    "value = pd.DataFrame(nn_pred)\n",
    "value.to_csv('./CV2/value2.csv', index=False)\n",
    "\n",
    "confusion = pd.DataFrame(labels)\n",
    "confusion.to_csv('./CV2/confusion2.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "945a8d18-fc30-4861-9512-3c301b8885f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.read_csv('data2/all.csv') \n",
    "alldata_coords = list(zip(alldata['Longitude'], alldata['Latitude']))\n",
    "alldata_coords = np.array(alldata_coords, dtype=np.float64)\n",
    "alldata_coords_x = np.array(alldata['Longitude'], dtype=np.float64)  # 提取'wls'列的数据并转换为 numpy 数组，横坐标单列\n",
    "alldata_coords_y = np.array(alldata['Latitude'], dtype=np.float64)\n",
    "dis_alldata = get_nn_dis(alldata_coords, data_coords_x, data_coords_y) # 强调是与已知label数据的data_coords_x,y计算distance\n",
    "\n",
    "x_temp = []\n",
    "for name in name_x:\n",
    "    x_temp.append(np.array(alldata[name], dtype=np.float64).reshape(-1, 1))\n",
    "x_all_2 = np.hstack(x_temp)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d60d13b-1a57-4f84-af50-9d27b0b44616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m113/113\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "# alldata['GNNWLR_pred']\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "beta_ols = np.array(pd.read_csv(f'./CV/weight/beta_ols_cv.csv'))\n",
    "gnnwlr_model = gnnwlr(beta_ols)\n",
    "gnnwlr_model.load_weights(f'./CV/weight/gnnwlr_wg_cv.weights.h5')\n",
    "\n",
    "nn_allpred = gnnwlr_model.predict((dis_alldata, x_all_2)).reshape(-1, 1)\n",
    "\n",
    "prevalue = pd.DataFrame(nn_allpred)\n",
    "alldata['GNNWLR_pred'] = prevalue\n",
    "prevalue\n",
    "df = pd.DataFrame(prevalue)\n",
    "\n",
    "df.to_csv('./prevalue.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ebd99e-8446-43cc-bcf1-9e80382f5098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m179/179\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "for i, name in enumerate(name_x): \n",
    "    temp = x_all_2.copy() \n",
    "    temp[:, i] = 0 \n",
    "    temp_pred = gnnwlr_model.predict((dis_alldata, temp)).reshape(-1, 1) \n",
    "    alldata[f\"lack_{name}\"] = temp_pred.squeeze().tolist() \n",
    "    alldata[f'SHAP_{name}'] = alldata['GNNWLR_pred'] - alldata[f\"lack_{name}\"] \n",
    "\n",
    "alldata\n",
    "df = pd.DataFrame(alldata)\n",
    "\n",
    "df.to_csv('C:/Users/SUST/Desktop/2050/585/allSHAP.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
